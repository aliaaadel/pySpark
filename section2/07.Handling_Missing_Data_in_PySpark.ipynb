{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data in PySpark\n",
    "\n",
    "In the real world, most datasets you work with will be incomplete, which means you will have missing data. You have 2 basic options for filling in missing data (you will personally have to make the decision for what is the right approach):\n",
    "\n",
    "1. Drop the missing data points (including the possbily the entire row)\n",
    "2. Fill them in with some other value (like the average).\n",
    "\n",
    "There are also two different types of missing data to be aware of:\n",
    "\n",
    "1. null values represents \"no value\" or \"nothing\", it's not even an empty string or zero. It can be used to represent that nothing useful exists. \n",
    "2. NaN stands for \"Not a Number\", it's usually the result of a mathematical operation that doesn't make sense, e.g. 0.0/0.0 \n",
    "\n",
    "Let's cover examples of each of these methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5ffde657c666:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>nulls</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f26782a54e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"nulls\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data for this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by reading a basic csv dataset\n",
    "# Let Spark know about the header and infer the Schema types!\n",
    "\n",
    "#Some csv data\n",
    "zomato = spark.read.csv('Datasets/zomato.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "This dataset contains the aggregate rating of restaurant in Bengaluru India from Zomato. \n",
    "\n",
    "**Source:** https://www.kaggle.com/himanshupoddar/zomato-bangalore-restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- online_order: string (nullable = true)\n",
      " |-- book_table: string (nullable = true)\n",
      " |-- rate: string (nullable = true)\n",
      " |-- votes: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- rest_type: string (nullable = true)\n",
      " |-- dish_liked: string (nullable = true)\n",
      " |-- cuisines: string (nullable = true)\n",
      " |-- approx_cost(for two people): string (nullable = true)\n",
      " |-- reviews_list: string (nullable = true)\n",
      " |-- menu_item: string (nullable = true)\n",
      " |-- listed_in(type): string (nullable = true)\n",
      " |-- listed_in(city): string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(zomato.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- online_order: string (nullable = true)\n",
      " |-- book_table: string (nullable = true)\n",
      " |-- rate: string (nullable = true)\n",
      " |-- votes: integer (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- rest_type: string (nullable = true)\n",
      " |-- dish_liked: string (nullable = true)\n",
      " |-- cuisines: string (nullable = true)\n",
      " |-- approx_cost(for two people): integer (nullable = true)\n",
      " |-- reviews_list: string (nullable = true)\n",
      " |-- menu_item: string (nullable = true)\n",
      " |-- listed_in(type): string (nullable = true)\n",
      " |-- listed_in(city): string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Edit some var types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = zomato.withColumn(\"approx_cost(for two people)\", zomato[\"approx_cost(for two people)\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"votes\", zomato[\"votes\"].cast(IntegerType()))\n",
    "#QA\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nulls values appear as \"None\" in the Pandas print out above. If we show the null values for the cuisines variable in attempt to view that first restaurant \"Jalsa\", we can see it appear as \"null\" below.  \n",
    "\n",
    "**search how to remove \\n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|           name|cuisines|\n",
      "+---------------+--------+\n",
      "|          Jalsa|    null|\n",
      "|  Grand Village|    null|\n",
      "|  Casual Dining|    null|\n",
      "|Timepass Dinner|    null|\n",
      "|  Casual Dining|    null|\n",
      "+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# zomato.filter(\"cuisines='None'\").agg(F.count(zomato.name)).show()\n",
    "df.filter(df.cuisines.isNull()).select(['name','cuisines']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Statistics\n",
    "\n",
    "It is always valualuable to know how much missing data you are going to be working with before you take any action like filling missing values with an average or dropping rows completly. Here is a good script to get you started. We will also explore more later on in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(df)\n",
    "spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way if you prefer\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "#first row: null count\n",
    "nulls = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "# Second row: null percent\n",
    "percent = df.select([format_number(((count(when(isnan(c) | col(c).isNull(), c))/df.count())*100),1).alias(c) for c in df.columns])\n",
    "\n",
    "result = nulls.union(percent)\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the missing data\n",
    "\n",
    "PySpark has a really handy .na function for working with missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that contains missing data across the whole dataset\n",
    "df.na.drop().limit(4).toPandas() \n",
    "\n",
    "# Note this statement is equivilant to the above:\n",
    "# df.na.drop(how='any').limit(4).toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course you will want to know how many rows that affected before you actually execute it..\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop().count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! 88% is a lot! We better figure out a better method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have at least 8 NON-null values\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(thresh=8).count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only drop the rows whose values in the sales column are null\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(subset=[\"votes\"]).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do the above\n",
    "og_len = df.count()\n",
    "drop_len = df.filter(df.rate.isNotNull()).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row only if ALL its values are null.\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(how='all').count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (character value)\n",
    "df.na.fill('MISSING').limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (numeric value)\n",
    "df.na.fill(999).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.isNull()).na.fill('No Name',subset=['name']).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common practice is to fill values with the **mean value** for the column. Here is a fun function to that in an automatted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(avg(c).alias(c) for c in df.columns if c in include))\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "updated_df = fill_with_mean(df, [\"votes\"])\n",
    "updated_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing values coniditonally (New - not in lecture YET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+\n",
      "|     fruit|quantity|  B|\n",
      "+----------+--------+---+\n",
      "|      Pear|      10|  1|\n",
      "|    Orange|      36|  1|\n",
      "|    Banana|     123|  1|\n",
      "|      Kiwi|    null|999|\n",
      "|     Peach|      16|  1|\n",
      "|Strawberry|       1|  0|\n",
      "+----------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When A is not null and A <5 then B = 0\n",
    "# When A is not null and A >= 5 then B=1\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "values = [('Pear',10),('Orange',36),('Banana',123),('Kiwi',None),('Peach',16),('Strawberry',1)]\n",
    "df = spark.createDataFrame(values,['fruit','quantity'])\n",
    "\n",
    "df.withColumn('B',(when(df.quantity.isNull(), 999).when(df.quantity < 5, 0).when(df.quantity >= 5, 1).otherwise(-2))).show()\n",
    "# df.withColumn('B',(when(df.quantity < 5, 0).when(df.quantity >= 5, 1).otherwise(999))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the missing data\n",
    "A few machine learning algorithms can easily deal with missing data. Just do your research and make sure the nulls values are not impacting the integrity of your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all we need to know for now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
